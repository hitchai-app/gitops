# GitLab Runner with Kubernetes Executor + Docker-in-Docker
#
# Architecture: Runner manager polls GitLab for jobs, spawns job pods dynamically
# Each job pod gets its own DinD service container for Docker builds/tests
#
# Resource Strategy (similar to ARC):
# - Runner manager: Small footprint (polls for jobs)
# - Job pods: Spawned on-demand with DinD sidecar
# - tmpfs volumes for Docker storage (RAM-backed for performance)

# GitLab server URL
gitlabUrl: https://gitlab.ops.last-try.org

# Environment variables for runner manager (cache credentials)
# Secret replicated from minio-infra via kubernetes-replicator
envVars:
  - name: CACHE_S3_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: minio-infra-config
        key: accesskey
  - name: CACHE_S3_SECRET_KEY
    valueFrom:
      secretKeyRef:
        name: minio-infra-config
        key: secretkey

# Runner token from sealed secret
# Create via: GitLab Admin > CI/CD > Runners > New instance runner
runners:
  secret: gitlab-runner-token

  # Runner configuration (TOML format)
  # Docs: https://docs.gitlab.com/runner/configuration/advanced-configuration.html
  config: |
    [[runners]]
      name = "kubernetes-dind"
      executor = "kubernetes"
      [runners.kubernetes]
        namespace = "gitlab-runner"
        image = "alpine:latest"
        privileged = true

        # Set DOCKER_HOST for docker:dind services
        # Services are accessible via their service name (e.g., tcp://docker:2376 for TLS)
        # Docs: https://docs.gitlab.com/runner/install/environment_variables_in_helm_charts.html
        environment = ["DOCKER_HOST=tcp://docker:2376"]

        # Poll interval for new jobs
        poll_interval = 3
        poll_timeout = 180

        # Image pull policy: use cached images if available
        # "if-not-present" = pull only if image not on disk (uses cache)
        # "always" = always pull from registry (default, slow with Docker Hub)
        # "never" = never pull, fail if image not cached
        pull_policy = "if-not-present"

        # Resource limits for job pods
        # Similar to ARC: Low requests for scheduling, high limits for bursting
        cpu_request = "250m"
        cpu_limit = "3000m"
        memory_request = "512Mi"
        memory_limit = "8Gi"

        # Service container limits (DinD)
        service_cpu_request = "100m"
        service_cpu_limit = "1000m"
        service_memory_request = "256Mi"
        service_memory_limit = "4Gi"

        # Helper container limits
        helper_cpu_request = "50m"
        helper_cpu_limit = "500m"
        helper_memory_request = "64Mi"
        helper_memory_limit = "512Mi"

        # Use emptyDir with Memory medium for Docker storage (RAM-backed)
        [[runners.kubernetes.volumes.empty_dir]]
          name = "docker-storage"
          mount_path = "/var/lib/docker"
          medium = "Memory"

        [[runners.kubernetes.volumes.empty_dir]]
          name = "docker-certs"
          mount_path = "/certs/client"
          medium = "Memory"

        [runners.kubernetes.pod_labels]
          "app.kubernetes.io/name" = "gitlab-runner-job"
          "gitlab-runner" = "true"

        # Environment variables for job containers
        [runners.kubernetes.pod_annotations]
          "cluster-autoscaler.kubernetes.io/safe-to-evict" = "false"

      # Distributed cache using MinIO (shared minio-infra tenant)
      # Credentials from environment variables (injected via envVars)
      [runners.cache]
        Type = "s3"
        Path = "runner"
        Shared = true
        [runners.cache.s3]
          ServerAddress = "minio.minio-infra.svc.cluster.local:80"
          BucketName = "gitlab-runner-cache"
          BucketLocation = "us-east-1"
          Insecure = true
          AccessKey = "$CACHE_S3_ACCESS_KEY"
          SecretKey = "$CACHE_S3_SECRET_KEY"

# Concurrent jobs
concurrent: 4

# Check interval for new jobs (seconds)
checkInterval: 10

# Unregister runner on shutdown (clean removal)
unregisterRunners: true

# Runner manager pod resources (not job pods)
# Note: Increased CPU limit from 200m to 400m to reduce throttling during job coordination
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 400m
    memory: 256Mi

# RBAC for runner manager
rbac:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["pods", "secrets", "configmaps", "serviceaccounts"]
      verbs: ["get", "list", "watch", "create", "patch", "update", "delete"]
    - apiGroups: [""]
      resources: ["pods/exec", "pods/attach", "pods/log"]
      verbs: ["get", "create"]

# Security context for runner manager (not job pods)
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  privileged: false
  capabilities:
    drop: ["ALL"]

podSecurityContext:
  runAsUser: 100
  fsGroup: 65533
