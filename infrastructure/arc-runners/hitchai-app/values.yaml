# GitHub Actions Runner Scale Set - hitchai-app
#
# RAM Optimization Strategy:
# - All volumes use tmpfs (medium: Memory) for maximum I/O performance
# - tmpfs is THIN-PROVISIONED: sizeLimit = upper bound, only actual writes consume RAM
# - Volume sizeLimits set below container memory limits (e.g., 10Gi docker-lib vs 12Gi container)
# - ResourceQuota (50Gi total) prevents runaway usage across all pods
# - Typical usage: 2-4Gi per pod, limits allow bursting to 12Gi for heavy builds

# Required: GitHub configuration
githubConfigUrl: "https://github.com/hitchai-app"
githubConfigSecret: "hitchai-app-github-app"

# CRITICAL: This name must match the runs-on label in workflows
# Defaults to Helm release name (hitchai-app-runners)
runnerScaleSetName: "hitchai-app-runners"

# Scaling
# Overcommit strategy: Low requests (512Mi) allow many pods to schedule
# ResourceQuota (50Gi limits.memory) caps total namespace usage to prevent node exhaustion
# CRITICAL: ResourceQuota enforces at limits level, not actual usage
# Current: 12Gi container limit × 4 runners = 48Gi (fits in 50Gi quota)
# To increase runners: Must increase ResourceQuota first
maxRunners: 4

# Runner pod template
# Documentation: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller
# NOTE: The ARC chart auto-populates DinD containers when containerMode.type=dind
# (see chart values comments); we manage the sidecar explicitly here instead.
template:
  spec:
    # ServiceAccount with read-only cluster access for kubectl operations
    # Used by Claude Code in GitHub Actions to verify deployments, check pod status, etc.
    serviceAccountName: github-actions-reviewer
    # In DinD mode (Kubernetes v1.29+), the dind container runs as an init container with restartPolicy: Always
    # This makes it a native Kubernetes sidecar that starts before and runs alongside the runner container
    # Reference: https://github.blog/changelog/2025-06-13-actions-runner-controller-0-12-0-release/
    initContainers:
    # First init container: copies runner externals (binaries/tools) to shared volume
    # This is required for the DinD container to access runner tools
    # Note: No resource limits set (matches upstream ARC behavior)
    # Runner externals size varies and can exceed 512Mi
    - name: init-dind-externals
      image: ghcr.io/actions/actions-runner:latest
      command: ["cp", "-r", "/home/runner/externals/.", "/home/runner/tmpDir/"]
      volumeMounts:
        - name: dind-externals
          mountPath: /home/runner/tmpDir

    # Second init container (sidecar): Docker daemon for workflow steps
    - name: dind
      image: docker:dind
      # Configure Docker daemon arguments
      # Documentation: https://docs.docker.com/reference/cli/dockerd/
      args:
        - dockerd
        # --host: Unix socket where Docker daemon listens
        # Required for runner container to communicate with Docker daemon via DOCKER_HOST env var
        - --host=unix:///var/run/docker.sock
        # --group: Sets GID for docker.sock to allow runner container access
        # The runner container runs with this GID to access the socket
        - --group=$(DOCKER_GROUP_GID)
        # --registry-mirror: Pull-through cache for Docker Hub images
        # First pull fetches from docker.io and caches locally, subsequent pulls served from cache
        # Mitigates Docker Hub rate limits (100 pulls/6h anonymous)
        # Documentation: https://docs.docker.com/docker-hub/mirror/
        - --registry-mirror=http://dockerhub-cache-docker-registry.registry-cache.svc.cluster.local:5000
      env:
        # GID 123 matches the docker group in the runner container
        # This allows the runner to access /var/run/docker.sock without being root
        - name: DOCKER_GROUP_GID
          value: "123"
      securityContext:
        # privileged: true required for DinD to function
        # Allows nested containerization (Docker daemon running in a container)
        privileged: true
      # restartPolicy: Always makes this init container a sidecar (runs throughout pod lifetime)
      # Without this, init container would exit after completion
      # Requires Kubernetes v1.29+ for native sidecar support
      restartPolicy: Always
      # Resource limits for Docker daemon
      # Docker daemon: 200-500MB base + layer caching (up to 4GB during builds)
      # RAM-optimized: All Docker storage (layers, cache) lives in RAM via docker-lib volume
      # Low requests ensure scheduling, high limits allow bursting
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "1000m"
          memory: "6Gi"  # Increased to accommodate RAM-based Docker storage
      # Startup probe ensures Docker daemon is ready before runner starts
      startupProbe:
        exec:
          command:
            - docker
            - info
        initialDelaySeconds: 0
        failureThreshold: 24
        periodSeconds: 5
      volumeMounts:
      - name: work
        mountPath: /home/runner/_work
      - name: dind-sock
        mountPath: /var/run
      - name: dind-externals
        mountPath: /home/runner/externals
      - name: docker-lib
        mountPath: /var/lib/docker  # Docker storage (layers, images, cache) in RAM

    containers:
    - name: runner
      image: ghcr.io/actions/actions-runner:latest
      command: ["/home/runner/run.sh"]
      # Overcommit strategy: Low requests for scheduling, high limits allow bursting
      # RAM-optimized: All volumes use tmpfs (thin-provisioned, only actual usage consumes RAM)
      # Per-pod: 512Mi requests → 16 pods fit in quota
      # Per-pod: 12Gi limits → Allows heavy builds, ResourceQuota caps total at 50Gi
      resources:
        requests:
          cpu: "250m"
          memory: "512Mi"
        limits:
          cpu: "3000m"
          memory: "12Gi"  # Increased to accommodate RAM-based work + Docker storage
      env:
        # DOCKER_HOST tells the runner where to find the Docker daemon
        # Points to the Unix socket shared with the dind sidecar
        - name: DOCKER_HOST
          value: unix:///var/run/docker.sock
        # Maximum time runner waits for Docker daemon to be ready
        - name: RUNNER_WAIT_FOR_DOCKER_IN_SECONDS
          value: "120"
      volumeMounts:
      - name: work
        mountPath: /home/runner/_work
      - name: dind-sock
        mountPath: /var/run

    volumes:
    # RAM-optimized volumes: All use tmpfs (medium: Memory) for performance
    # tmpfs is THIN-PROVISIONED: sizeLimit is upper bound, only actual writes consume RAM
    # Volume sizeLimits must be ≤ container memory limit (tmpfs counts toward container usage)
    # DinD container: 12Gi limit, so docker-lib + overhead ≤ 12Gi
    # ResourceQuota (50Gi total) prevents runaway usage across all pods

    - name: work
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi  # Workflow workspace

    - name: docker-lib
      emptyDir:
        medium: Memory
        sizeLimit: 10Gi  # Docker images, layers, build cache (leaves 2Gi headroom for daemon)

    - name: dind-sock
      emptyDir:
        medium: Memory
        sizeLimit: 100Mi  # Unix socket for Docker communication

    - name: dind-externals
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi  # Runner binaries and tools (increased from 500Mi - actual size varies)

# Explicitly link this scale set to the controller the chart failed to discover automatically.
controllerServiceAccount:
  namespace: arc-systems
  name: arc-gha-rs-controller

# Listener pod template configuration
# Reference: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller
# CRITICAL: Do NOT change the container name "listener" - it must match exactly or the config will be treated as a sidecar
listenerTemplate:
  # Enable Prometheus metrics scraping
  # Reference: https://github.com/actions/actions-runner-controller/discussions/3786
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: "/metrics"
      prometheus.io/port: "8080"
  spec:
    containers:
      # CRITICAL: Container name MUST be "listener" - changing it will create a sidecar instead
      # Reference: https://github.com/actions/actions-runner-controller/blob/master/charts/gha-runner-scale-set/values.yaml
      - name: listener
        # Expose metrics endpoint for Prometheus scraping and health probes
        # The listener container serves metrics on port 8080 at /metrics
        ports:
          - containerPort: 8080
            name: metrics
            protocol: TCP
        # Health probes to detect and recover from stuck listener state
        # Context: Listener can get stuck polling the same messageID without processing new jobs
        # Related issues:
        # - https://github.com/actions/actions-runner-controller/issues/3100 (Listener randomly turns non-responsive)
        # - https://github.com/actions/actions-runner-controller/issues/3204 (Scale Set Listener Stops Responding)
        # - https://github.com/actions/actions-runner-controller/issues/3005 (Listener stuck offline but pod running)
        # - https://github.com/actions/runner/issues/3487 (BrokerMigration loop, no jobs picked up)
        #
        # These probes check the /metrics endpoint (port 8080) exposed by the listener.
        # While this doesn't directly detect "stuck polling same messageID", it provides basic health monitoring.
        # For comprehensive monitoring, consider alerting on listener metrics stalling (no new jobs processed).
        livenessProbe:
          httpGet:
            path: /metrics
            port: 8080
          initialDelaySeconds: 60      # Wait 1 minute for listener to establish GitHub connection
          periodSeconds: 60            # Check every minute
          timeoutSeconds: 10           # Allow 10 seconds for response
          failureThreshold: 5          # Restart after 5 consecutive failures (5 minutes total)
        readinessProbe:
          httpGet:
            path: /metrics
            port: 8080
          initialDelaySeconds: 30      # Check readiness after 30 seconds
          periodSeconds: 30            # Check every 30 seconds
          timeoutSeconds: 5            # Allow 5 seconds for response
          failureThreshold: 3          # Mark unhealthy after 3 consecutive failures (1.5 minutes)
