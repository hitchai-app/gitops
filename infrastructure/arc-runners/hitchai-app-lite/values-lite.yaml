# GitHub Actions Runner Scale Set - hitchai-app-lite
#
# Lightweight Runner Strategy (NO Docker-in-Docker):
# - Minimal resource footprint: 4Gi runner container only
# - For workflows that don't need Docker (linting, unit tests, type checks)
# - 12 concurrent runners for maximum parallelism
# - Per-pod: ~4Gi limits (dramatically lighter than 18Gi heavy runners)
# - Use heavy runners for workflows requiring Docker builds

# Required: GitHub configuration
githubConfigUrl: "https://github.com/hitchai-app"
githubConfigSecret: "hitchai-app-github-app"

# CRITICAL: This name must match the runs-on label in workflows
# Use this in workflows with: runs-on: hitchai-app-runners-lite
runnerScaleSetName: "hitchai-app-runners-lite"

# Scaling
# Lightweight runner strategy: 12 concurrent runners (no DinD overhead)
# Per-pod: ~4Gi limit (runner only)
# 12 runners Ã— 4Gi = 48Gi (38% of 128GB cluster)
maxRunners: 12

# Runner pod template
# Documentation: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller
# NOTE: No DinD - workflows requiring Docker must use heavy runners (hitchai-app-runners)
template:
  spec:
    # ServiceAccount with read-only cluster access for kubectl operations
    # Used by Claude Code in GitHub Actions to verify deployments, check pod status, etc.
    serviceAccountName: github-actions-reviewer
    containers:
    - name: runner
      image: ghcr.io/actions/actions-runner:latest
      command: ["/home/runner/run.sh"]
      # Lightweight runner strategy: Minimal limits for maximum parallelism
      # No Docker daemon overhead - suitable for linting, unit tests, type checking
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "1000m"
          memory: "4Gi"
      volumeMounts:
      - name: work
        mountPath: /home/runner/_work

    volumes:
    # Minimal volume configuration - just workflow workspace
    - name: work
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi  # Workflow workspace

# Explicitly link this scale set to the controller the chart failed to discover automatically.
controllerServiceAccount:
  namespace: arc-systems
  name: arc-gha-rs-controller

# Listener pod template configuration
# Reference: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller
# CRITICAL: Do NOT change the container name "listener" - it must match exactly or the config will be treated as a sidecar
listenerTemplate:
  # Enable Prometheus metrics scraping
  # Reference: https://github.com/actions/actions-runner-controller/discussions/3786
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/path: "/metrics"
      prometheus.io/port: "8080"
  spec:
    containers:
      # CRITICAL: Container name MUST be "listener" - changing it will create a sidecar instead
      # Reference: https://github.com/actions/actions-runner-controller/blob/master/charts/gha-runner-scale-set/values.yaml
      - name: listener
        # Health probes to detect and recover from stuck listener state
        # Context: Listener can get stuck polling the same messageID without processing new jobs
        # Related issues:
        # - https://github.com/actions/actions-runner-controller/issues/3100 (Listener randomly turns non-responsive)
        # - https://github.com/actions/actions-runner-controller/issues/3204 (Scale Set Listener Stops Responding)
        # - https://github.com/actions/actions-runner-controller/issues/3005 (Listener stuck offline but pod running)
        # - https://github.com/actions/runner/issues/3487 (BrokerMigration loop, no jobs picked up)
        #
        # These probes check the /metrics endpoint (port 8080) exposed by the listener.
        # While this doesn't directly detect "stuck polling same messageID", it provides basic health monitoring.
        # For comprehensive monitoring, consider alerting on listener metrics stalling (no new jobs processed).
        livenessProbe:
          httpGet:
            path: /metrics
            port: 8080
          initialDelaySeconds: 60      # Wait 1 minute for listener to establish GitHub connection
          periodSeconds: 60            # Check every minute
          timeoutSeconds: 10           # Allow 10 seconds for response
          failureThreshold: 5          # Restart after 5 consecutive failures (5 minutes total)
        readinessProbe:
          httpGet:
            path: /metrics
            port: 8080
          initialDelaySeconds: 30      # Check readiness after 30 seconds
          periodSeconds: 30            # Check every 30 seconds
          timeoutSeconds: 5            # Allow 5 seconds for response
          failureThreshold: 3          # Mark unhealthy after 3 consecutive failures (1.5 minutes)
