# CloudNativePG Deployment Verification

This document provides verification commands for the CloudNativePG operator and PostgreSQL cluster deployment.

## Prerequisites

1. Kubernetes cluster running
2. ArgoCD installed and configured
3. Longhorn storage deployed with `longhorn-single-replica` StorageClass
4. S3 bucket created: `cloudnativepg-backups`
5. S3 credentials created as SealedSecrets (see README.md)

## Deployment Order

1. CloudNativePG operator (via ArgoCD)
2. S3 credentials SealedSecrets (manual)
3. PostgreSQL clusters (via ArgoCD)

## Verification Commands

### 1. Verify CloudNativePG Operator

```bash
# Check operator deployment
kubectl get deployment -n cnpg-system
kubectl get pods -n cnpg-system

# Expected output:
# NAME                              READY   STATUS    RESTARTS   AGE
# cnpg-cloudnative-pg-xxxx          1/1     Running   0          5m

# Check operator logs
kubectl logs -n cnpg-system deployment/cnpg-cloudnative-pg

# Verify CRDs installed
kubectl get crd | grep postgresql.cnpg.io

# Expected CRDs:
# backups.postgresql.cnpg.io
# clusters.postgresql.cnpg.io
# poolers.postgresql.cnpg.io
# scheduledbackups.postgresql.cnpg.io
```

### 2. Verify ArgoCD Applications

```bash
# Check ArgoCD applications
kubectl get application -n argocd | grep -E 'cloudnativepg|postgres'

# Expected output:
# cloudnativepg    Synced   Healthy   ...
# postgres-stage   Synced   Healthy   ...
# postgres-prod    Synced   Healthy   ...

# Detailed status
kubectl describe application cloudnativepg -n argocd
kubectl describe application postgres-stage -n argocd
kubectl describe application postgres-prod -n argocd
```

### 3. Verify PostgreSQL Clusters

```bash
# Stage cluster
kubectl get cluster -n postgres-stage
kubectl describe cluster postgres-shared -n postgres-stage

# Production cluster
kubectl get cluster -n postgres-prod
kubectl describe cluster postgres-shared -n postgres-prod

# Expected cluster status fields:
# - Phase: Cluster in healthy state
# - Instances: 1
# - Ready Instances: 1
# - Current Primary: postgres-shared-1
```

### 4. Verify Pods

```bash
# Stage pods
kubectl get pods -n postgres-stage
kubectl logs -n postgres-stage postgres-shared-1 -c postgres | tail -50

# Production pods
kubectl get pods -n postgres-prod
kubectl logs -n postgres-prod postgres-shared-1 -c postgres | tail -50

# Expected pod status:
# NAME               READY   STATUS    RESTARTS   AGE
# postgres-shared-1  1/1     Running   0          5m
```

### 5. Verify Storage

```bash
# Check PVCs
kubectl get pvc -n postgres-stage
kubectl get pvc -n postgres-prod

# Expected PVC:
# NAME                       STATUS   VOLUME    CAPACITY   STORAGE CLASS
# postgres-shared-1          Bound    pvc-xxx   10Gi       longhorn-single-replica

# Verify Longhorn volumes
kubectl get volume -n longhorn-system | grep postgres
```

### 6. Verify S3 Backup Configuration

```bash
# Check if S3 credentials secret exists
kubectl get secret postgres-backup-s3-credentials -n postgres-stage
kubectl get secret postgres-backup-s3-credentials -n postgres-prod

# Test S3 connectivity from pod
kubectl exec -it -n postgres-stage postgres-shared-1 -- bash -c "
  export AWS_ACCESS_KEY_ID=\$(cat /controller/secrets/postgres-backup-s3-credentials/ACCESS_KEY_ID)
  export AWS_SECRET_ACCESS_KEY=\$(cat /controller/secrets/postgres-backup-s3-credentials/ACCESS_SECRET_KEY)
  barman-cloud-backup-list s3://cloudnativepg-backups/postgres-stage
"
```

### 7. Verify Database Connectivity

```bash
# Get app user password (auto-generated by operator)
export PGPASSWORD=$(kubectl get secret -n postgres-stage postgres-shared-app -o jsonpath='{.data.password}' | base64 -d)

# Connect to database
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U app -d app

# Run test query
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U app -d app -c "SELECT version();"
```

### 8. Verify Monitoring

```bash
# Check PodMonitor
kubectl get podmonitor -n postgres-stage
kubectl get podmonitor -n postgres-prod

# Check Prometheus targets (if Prometheus installed)
kubectl port-forward -n monitoring svc/prometheus 9090:9090
# Open http://localhost:9090/targets and search for "cnpg"
```

### 9. Trigger Manual Backup (Optional)

```bash
# Create on-demand backup
cat <<EOF | kubectl apply -f -
apiVersion: postgresql.cnpg.io/v1
kind: Backup
metadata:
  name: postgres-shared-manual-backup
  namespace: postgres-stage
spec:
  cluster:
    name: postgres-shared
EOF

# Check backup status
kubectl get backup -n postgres-stage
kubectl describe backup postgres-shared-manual-backup -n postgres-stage

# Verify in S3
aws s3 ls s3://cloudnativepg-backups/postgres-stage/base/
```

## Health Checks

### Cluster Health

```bash
# Check cluster overall health
kubectl get cluster -n postgres-stage -o jsonpath='{.items[0].status.phase}'
# Expected: "Cluster in healthy state"

# Check ready instances
kubectl get cluster -n postgres-stage -o jsonpath='{.items[0].status.readyInstances}'
# Expected: 1

# Check current primary
kubectl get cluster -n postgres-stage -o jsonpath='{.items[0].status.currentPrimary}'
# Expected: postgres-shared-1
```

### Replication Health (when scaled to multi-node)

```bash
# Check replication status
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SELECT application_name, state, sync_state, replay_lag
  FROM pg_stat_replication;
"
```

### WAL Archiving Status

```bash
# Check WAL archiving
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SELECT * FROM pg_stat_archiver;
"

# Expected: archived_count increasing, last_failed_time should be NULL
```

## Troubleshooting

### Operator Not Starting

```bash
# Check operator logs
kubectl logs -n cnpg-system deployment/cnpg-cloudnative-pg --tail=100

# Common issues:
# - Missing CRDs: kubectl apply -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.27/config/crd/bases/
# - RBAC issues: Check ClusterRole and ClusterRoleBinding
```

### Cluster Stuck in "Setting up primary"

```bash
# Check cluster events
kubectl describe cluster postgres-shared -n postgres-stage

# Check pod logs
kubectl logs -n postgres-stage postgres-shared-1 -c postgres

# Common issues:
# - StorageClass not found: Verify longhorn-single-replica exists
# - PVC pending: Check Longhorn health
# - Init container failed: Check image pull or permissions
```

### Backup Failures

```bash
# Check backup logs
kubectl logs -n postgres-stage postgres-shared-1 -c postgres | grep barman

# Verify S3 credentials
kubectl get secret postgres-backup-s3-credentials -n postgres-stage -o yaml

# Test S3 access manually
kubectl exec -it -n postgres-stage postgres-shared-1 -- bash
export AWS_ACCESS_KEY_ID=xxx
export AWS_SECRET_ACCESS_KEY=xxx
barman-cloud-check-wal-archive s3://cloudnativepg-backups/postgres-stage
```

### High Memory Usage

```bash
# Check PostgreSQL memory settings
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SHOW shared_buffers;
  SHOW effective_cache_size;
  SHOW maintenance_work_mem;
"

# Adjust in cluster.yaml if needed:
# shared_buffers should be ~25% of pod memory limit
# effective_cache_size should be ~50% of pod memory limit
```

## Common Error Messages

| Error | Cause | Solution |
|-------|-------|----------|
| `failed to create volume` | StorageClass not found | Verify longhorn-single-replica exists |
| `backup failed: S3 access denied` | Invalid S3 credentials | Recreate SealedSecret with correct credentials |
| `cluster stuck in "Creating primary instance"` | Init container failure | Check logs: `kubectl logs postgres-shared-1 -c bootstrap-controller` |
| `WAL archiver failed` | S3 connectivity issue | Test S3 access from pod, check network policies |

## Performance Baseline

After successful deployment, record baseline metrics:

```bash
# Connection count
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SELECT count(*) FROM pg_stat_activity;
"

# Database size
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SELECT pg_size_pretty(pg_database_size('app'));
"

# Cache hit ratio
kubectl exec -it -n postgres-stage postgres-shared-1 -- psql -U postgres -c "
  SELECT
    sum(heap_blks_read) as heap_read,
    sum(heap_blks_hit) as heap_hit,
    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
  FROM pg_statio_user_tables;
"
```

## Success Criteria

- [ ] CloudNativePG operator pod running in cnpg-system
- [ ] CRDs installed (clusters, backups, etc.)
- [ ] ArgoCD Applications synced and healthy
- [ ] PostgreSQL pods running (stage and prod)
- [ ] PVCs bound with longhorn-single-replica
- [ ] Database accessible via psql
- [ ] S3 backup credentials configured
- [ ] Manual backup succeeds
- [ ] Monitoring PodMonitor created
- [ ] No error messages in cluster status

## Next Steps

After verification:

1. Create product Database CRDs (see README.md)
2. Configure application connection strings
3. Set up Grafana dashboards for PostgreSQL metrics
4. Test disaster recovery procedures
5. Document backup restoration process

## References

- [CloudNativePG Troubleshooting](https://cloudnative-pg.io/documentation/current/troubleshooting/)
- [PostgreSQL Monitoring Queries](https://wiki.postgresql.org/wiki/Monitoring)
- Infrastructure README: [README.md](./README.md)
